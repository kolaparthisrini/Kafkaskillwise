https://share.internxt.com/d/sh/folder/3647eb2ec03b928035b3/c958b723a5d30de54c016c42935b3d757ac4893b8fd5e185b9b87e22d6aa2204
https://github.com/kolaparthisrini

KAfka:
============================
To initiate kafka on local machine,we needd to follow below steps.
1)Download zookeeper server.
2)make necessary modifications in server config files,if necessary
3)start the server and check wwhther server is running on the specified port.
4)Download Kafka server to any drive except downloads folder.
note: dont launch any server from downloads folder.
5)Make necessary changes to kafka config and launch the kafka server parallely with zookeeper.
=============================
DAy2
----------------
Apache kafka or confluence kafka(https://www.confluent.io/get-started/)
=>Apache kafka is open source and confluence kafka is cloud based and which is commercial for development.fl
confluence comes with 2 types of licensing 1)confluence platform --free community edition
                                           2)confluence platform enterprise---costly---mainly for production environments.
==> apache kafka has limited connectors and confluence has more no of connectors.
==>no inbuilt tools for kafka,but confluence is power packed with multiple tools and GUI is available.
=>security wise confluence is stronger.
=>kafka does not have load balancers, but confluent has inbuilt load balancing.
---------------------------------------------------------
In how many ways can we install kafka?
1)on-premises
2)cloud---for cloud installations,we use AWS,GCP,AZURE,Alibab cloud ,Confluence...etc
3)docker
4)vm
====>
We need a server.
IN IT,we do have 2 types of servers.
1)Web Server-----Nginx,Tomcat,jetty ....etc

Web server is software with below components.
Web server= Http Engine + Catalina container+Jasper container+Middleware services

2)Application server-------Weblogic,jboss,pramathi,Glashfish.......etc
App server is software with below components.
App server= Http Engine + Catalina container+Jasper container+MQ(messsaging queue) container+EJB(Enterprise java beans)
container+Middleware services

browser is called as "User-agent"
>>Http is a stateless protocol.All web browsers use Http/https Protocol.
What is commercial cert in https?

http will not provide security.
Https= http+SSl layer

to enable ssl at server level,we need to use security certificates.There are 2 types of security certificates.
1)self signed: self signed certs are the certs that we are going to create,ie any human can create it manullay using 
java commands.These are not secure.we can use self for local testing.
2)Commercial: certs thats are provided by vendors like go-daddy,verisign,cloudfare,paypal,paytmcerts,CA....etc.
every certificate will have private keys,validity period,security algo and cert-chain,issueirs details.

For production env and for dev ,we need to use commerial.
==============================================>
I wan to start my kafka server?
We had installed offset software for monitoring.
------------------------------>
Connector:A connector is a software which is used for transferring data from source/origin point to destination.

in the market we do have so many connectors.For kafka,we can use "kafka connector".

java connector.For working on differnt types of connectors,you need to learn ESB which 
is Mulesoft,Apache camel,tibco,sap...

ESB is a architecture---Stands for enterprise service bus.ESB comes with some design patterns for
 which vendors like mule,tibco,apachecame up with implementations.
 
 In short is is group of rules ,regulations which are mainly for integrating multiple apps which are heterogenious.

Broker/Mediator: Broker means a node or a server or an instance or machine
Transformation:converting data from one form to another.
Example: xml to json convertion and vice versa
---------------
Producer:any thing which emits or gives data to destination is called as producers.
a producer can be an application,device/IOT device,sensors,api,log file,webservice,db or any data file...etc.

youtube-----streaming or live streaming

Consumer:who will accept data.
a consumer can be a device,webservice,app,server,machine...etc.


Queue: queue is a data structure/storage area/db with the priciple, FIFO---first in first out.
To implement QUEUE ,we need one producer and only one consumer
P----M---Q---R

Receiver once received he need to send and Acknowledgement.

Every queue has policys.In this policy(xml/json) we are going define how many days message shld stay in queue.

Message: any thing which is in binary format.Message can be of any format like xml,json,bson...etc
Topic:Topic can store your data temporarily like queue.To implement Topic ,we can have multiple producers and multiple  consumers.

producer----------->QUEUE------------>Consumer
producer----------->TOPIC------------>Consumer1
						  ----------->Consumer2	
						 
Apache kafa follows topic implementation.
==>Cluster: group of brokers/broker instances is called as cluster.
As per the kafka best practices,in a cluster we need have a minimum of 3 brokers or even need to have odd no of brokers
in real time .
->kafka is stateless.So it uses Zookeeper to state management.

Meta data:any thing which describes an object is called meta data.ie its name,shaze,size...etc.
What is Event?
do u have listner?listner is a s/w pgm which responds to events.
Consumer group: group of consumers are called as consumer group.Every group has a name which is userdefined.
Producer group: group of producers are called as producer group.Every group has a name which is userdefined.

always group of producers shld have some unique name or ID
always group of Consumers shld have some unique name or ID


use of TCP protocol? it is a standard for secure data transmission between client and server.
kafka uses tcp protocol.

Encryption?
decryption?

can i perform encryption in one machine and decryption in another machine?
ans) no
encryption algo will change from machine to machine.so encryption cannot be done in one  machine and
 decryption cannot be done in another machine.
 
 compressing techniqs: use gzip and deflatter techniqs

replication means data duplication.
Stateless protocol:
when ever we request fb.com/google  from browser,what will happen?
=======================================
Day3
----------------
In networking,to make a cluster we need to have atleast 1 node.
In kafka,to make a cluster as per the best practices,we need to have a min of 3 or odd no.


Apache nifi:
used for data routing ,tranformations(example: xml to json or vice versa) and analytics.

Autoscaling:  in othe cloud computing vendor dynamically adjust the resources based on demand from customers.
Note:autoscaling feature is available mainly in clouds like AWS,azure,GCP....etc.

kafka Architectures
========================
Day4
-----------------------------
Devops or devsecops

Stream: data in motion or continous flow of data .Data transmission will occur in bytes from source to destination.

Observations and resolutions today:
........................................
after the session,associates are not closing the zookeeper and kafka properly.
sol) Close the servers in below order:
d/sh/folder/3647eb2ec03b928035b3/c958b723a5d30de54c016c42935b3d757ac4893b8fd5e185b9b87e22d6aa2204
Closing Order : First disconnect from cluster in Offset tool and then close it
                                                     --->Producer
													    --->Consumer
														      --->Kafka
															       --->zookeeper
																     ---->RD

Dont perform hard exit.PErform graceful exit.

Log folder path locations are invalid.:Use \\ in the place of \.
always create folder for logs and update the same in config files.

10PM
2:13PM---BROKER1 CRASHED

BUT MORE OF CONSUMERS WANT DATA AT 2:15PM.IF U PROVIDE 24/7 AVAILABILITY OF TOPICS,THEN UR BUSINESS CAN BE GROWN.

IF REPLICATION IS 3,THEN HOW MANY BROKERS ARE NEEDED?
ANS)3
IF REPLICATION IS 20,THEN HOW MANY BROKERS ARE NEEDED?
ANS)20

REPLICATION IS COSTLY PROCESS.

KAFKA IS fault TOLERANT.
ZOOKEEPER-----ETCD,DOCKER,HASICORP,HYSTRIX,CONSUL....

iN LATEST VERSIONS OF KAFKA,IE 3.X VERSIONS ONWARDS,WE CAN USE "kraft" AS A REPLACE MENT FOR ZOOKEEPER.
-------------------------------------------------------------------------------------
Day5
========
>The connector that takes data from a Producer and feeds them into a topic is called source connector. The connector 
that takes data from a Topic and delivers them to a Consumer is called Sink Connector.
>Single Node cluster: a cluster with single node/broker in ur local machine.
>Multi-Node cluster:a cluster with more than 1 node/broker in ur in your network.
when data is huge,
multiple consumers and topics are needed
need more replicas for fault tolerant
>Multi-node cluster can be created in a single machine or in multiple machines or on cloud and then create a cluster
>by default "tmp"
 location will be  enabled in kafka.It is devops engineers responsibilityt to change from default location.
>Every broker will have an id.Bydefault broker will have id as 0.We can change the id as per requirement.
>No 2 brokers can have same id.
>By default brokwer will run on port no 9092.Wan modify as per ur project.
0007----wrong
8989
8000,7000
Always port no shld be of 4 digits.
>By default tmp folder is used by your os.
D:\\Kafka\\Multi_node\\zookeeper_logs
An in-sync replica (ISR) is a broker that has the latest data for a given partition.

kafka-topics.bat --zookeeper localhost:9093,localhost:7094 --describe --topic srinivas

----------------------------
Requirement1:

I am having some data in file 1.i want to transfer trhe data from file 1 to topic "file" and from there
data shld be transfred to consumer "cons"

producer: file1.txt--------->Topic:file1------>consumer:output.txt
=====================
>,<,/,\,||,|---are called as special operators or pipes in os.This commands differs from os to os based on manufaturer or vendor.

kafka connector
cin<<
cout>>
------------------
partitions and segments
Zookeeper algo and its role
replication exercisec
------------------

Monday: policies,kafka theoryie configs of broker and topic
Tues: security and application creations

if you have N + 1 consumers for a topic with N partitions, then the first N consumers will be assigned a partition, and the remaining consumer will be idle, unless one of the N consumers fails, then 
the waiting consumer will be assigned its partition. This is a good strategy to implement a hot failover.
---------------
# Partitions = Desired Throughput / Partition Speed
If your consumer group had 3 consumers and you add one new consumer then it will sit ideal. No of consumers in consumer-group <= number of partitions.
kafka-console-consumer --bootstrap-server localhost:9092 --topic insurance --from-beginning --group group 1
==================================================================
Day 6
--------------------
Multi node: In multinode, nodes can be in same location/pc or in different pc/cluster.
if nodes are across n/ws,how ddo we monitor?
Jvisual vm(JMX)/java melody can be used to monitor local cluster/single node cluster.
datadog/appdynamics/conducktor can be used to monitor multinode cluster.
quoram is for zookeeper.
>We need to install conducktor for desktop edition.

ISR,replication factor

Rf shld be equal to brokers.

In networking,incoming request is called as "ingress" and outgoing response is called as "engress"
>what if leader is crashed because of technical reasons?
ans)one of the slave shld be act as leader.
Now the question is who will decide the master?
how do kafa now how many slaves are active and how many are dead/crashed/not working?
who will scan ingress traffic?
Who will deligate traffic to slatves?
He will also load balancing.he can act as Gateway to ur cluster
for all above,ans is "ZOOKEEPER".
zookeeper performs node management.zookeeper uses election algorithms to elect a node as a leader.

zookeeper maintains meta data of leader and slaves in hashmap.

slave info means ipaddress,cluster id,slave name,status.

zookeeper uses heartbeat signals to find whether the slaves are active or dead.
zookeeper send heart beat signals for every 100ms.

for election algorithm,inputs will be ipaddress,turnaround time,nearest node,mac,active time of node,traffic ...etc
We need to use "WebUI" tools.
>>group of zookeeper nodes is called as "Ensemble".In zookeeper eack node called as znode.
===================
===>Within Same  group:
2  consumers within same group cannot consume same message from partition.

con1,con2---grp1
con3,con4----grp2

===>Within different  group:

2 consumers in 2 groups can consume the same message from partition.
























































































